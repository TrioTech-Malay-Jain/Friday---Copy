

----- File: C:\Users\int10281\Desktop\Github\Friday - Copy\all_code_snippets.txt -----


----- File: C:\Users\int10281\Desktop\Github\Friday - Copy\cagent.py -----
import os
from dotenv import load_dotenv
import logging
import asyncio

from livekit import agents
from livekit.agents import AgentSession, Agent, RoomInputOptions, RoomOutputOptions, JobContext
from livekit.plugins import google, cartesia, deepgram, noise_cancellation, silero

from prompts import AGENT_INSTRUCTION, SESSION_INSTRUCTION
from tools import get_weather, search_web
import config

load_dotenv()
logging.basicConfig(level=logging.INFO)


class Assistant(Agent):
    def __init__(self):
        super().__init__(
            instructions=AGENT_INSTRUCTION,
            llm=google.LLM(model="gemini-1.5-flash", temperature=0.8),
            tools=[get_weather, search_web],
        )


async def entrypoint(ctx: JobContext):
    # Setup conversation logging
    config.setup_conversation_log()
    
    agent = Assistant()

    session = AgentSession(
        stt=deepgram.STT(model="nova-3", language="multi"),
        llm=google.LLM(model="gemini-1.5-flash", temperature=0.8),
        tts=cartesia.TTS(
            model="sonic-2",
            language="hi",
            voice="f91ab3e6-5071-4e15-b016-cde6f2bcd222",
        ),
        vad=silero.VAD.load(),
    )

    await session.start(
        room=ctx.room,
        agent=agent,
        room_input_options=RoomInputOptions(noise_cancellation=noise_cancellation.BVC()),
        room_output_options=RoomOutputOptions(audio_enabled=True),
    )

    await session.generate_reply(instructions=SESSION_INSTRUCTION)
    await asyncio.Future()  # Run until externally stopped


if __name__ == "__main__":
    agents.cli.run_app(agents.WorkerOptions(entrypoint_fnc=entrypoint))


----- File: C:\Users\int10281\Desktop\Github\Friday - Copy\config.py -----
import os
import datetime

_conversation_log_path = None

def set_conversation_log_path(path: str):
    global _conversation_log_path
    _conversation_log_path = path

def get_conversation_log_path() -> str:
    if _conversation_log_path is None:
        raise RuntimeError("Conversation log path not set!")
    return _conversation_log_path

def setup_conversation_log():
    """Setup conversation log file path and create directory if needed"""
    log_dir = os.path.join(os.getcwd(), "conversations")
    os.makedirs(log_dir, exist_ok=True)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_path = os.path.join(log_dir, f"conversation_{timestamp}.json")
    set_conversation_log_path(log_path)
    
    # Initialize empty conversation file
    import json
    with open(log_path, "w", encoding="utf-8") as f:
        json.dump({"conversation": []}, f, ensure_ascii=False, indent=2)
    
    return log_path

----- File: C:\Users\int10281\Desktop\Github\Friday - Copy\copy_utils.py -----
import os

# Root directory to start searching from
ROOT_DIR = r"C:\Users\int10281\Desktop\Github\Friday - Copy"

# File extensions to include
INCLUDE_EXTENSIONS = {'.py', '.js', '.css', '.html', '.txt'}

# Output file where combined code will be saved
OUTPUT_FILE = os.path.join(ROOT_DIR, "all_code_snippets.txt")

def should_include(file_name):
    return os.path.splitext(file_name)[1] in INCLUDE_EXTENSIONS

def copy_code_snippets():
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as outfile:
        for dirpath, dirnames, filenames in os.walk(ROOT_DIR):
            # Exclude venv folder from traversal
            if 'venv' in dirnames:
                dirnames.remove('venv')
            if 'KMS' in dirnames:
                dirnames.remove('KMS')
            if '__pycache__' in dirnames:
                dirnames.remove('__pycache__')
            if 'conversations' in dirnames:
                dirnames.remove('conversations')
            # if 'copy_utils.py' in filenames:
            #     filenames.remove('copy_utils.py')
            # if 'agent.py' in filenames:
            #     filenames.remove('agents.py')

            for filename in filenames:
                if should_include(filename):
                    file_path = os.path.join(dirpath, filename)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            outfile.write(f"\n\n----- File: {file_path} -----\n")
                            outfile.write(content)
                    except Exception as e:
                        print(f"Could not read {file_path}: {e}")

    print(f"\n✅ Code snippets copied to: {OUTPUT_FILE}")

if __name__ == "__main__":
    copy_code_snippets()


----- File: C:\Users\int10281\Desktop\Github\Friday - Copy\frontend.html -----
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Friday Assistant</title>
    <!-- Google Font: Inter -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <!-- Font Awesome for Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha512-mbkBd5Ge7s8QDJVc+nRBD3GKjqNsG9hs2jS2YLEKmlZWFAP7C1AqRQzyYSm6kI6hXFxiB3RR/d3k5xn5YgLYw==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
        /* CSS for the minimalist dark-mode design and responsive layout */
        :root {
            --bg-color: #1a1a2e;
            --text-color: #e0e0ef;
            --primary-color: #007bff;
            --secondary-color: #4b89dc;
            --card-bg: #2a2a44;
            --border-radius: 12px;
            --font-family: 'Inter', sans-serif;
        }

        body {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: var(--font-family);
            background-color: var(--bg-color);
            color: var(--text-color);
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            flex-direction: column;
            overflow: hidden;
            text-align: center;
        }

        .container {
            width: 90%;
            max-width: 800px;
            display: flex;
            flex-direction: column;
            justify-content: space-between;
            align-items: center;
            height: 90vh;
            padding: 20px;
        }

        /* --- Assistant's Wave Visualizer --- */
        #wave-container {
            flex-grow: 1;
            width: 100%;
            display: flex;
            justify-content: center;
            align-items: center;
        }

        #wave-canvas {
            width: 100%;
            height: 200px;
            opacity: 0.7;
        }

        /* --- Transcription Display --- */
        #transcription-display {
            width: 100%;
            height: 150px;
            margin: 20px 0;
            padding: 15px;
            background-color: var(--card-bg);
            border-radius: var(--border-radius);
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.3);
            overflow-y: auto;
            text-align: left;
            font-size: 1rem;
            line-height: 1.6;
        }
        
        #transcription-display::-webkit-scrollbar {
            width: 8px;
        }
        
        #transcription-display::-webkit-scrollbar-track {
            background: #333;
            border-radius: 10px;
        }
        
        #transcription-display::-webkit-scrollbar-thumb {
            background: var(--primary-color);
            border-radius: 10px;
        }

        /* --- User Input Area --- */
        .input-area {
            width: 100%;
            display: flex;
            align-items: center;
            gap: 15px;
            background-color: var(--card-bg);
            padding: 10px;
            border-radius: var(--border-radius);
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.3);
        }

        #text-input {
            flex-grow: 1;
            padding: 12px;
            border: none;
            background: #3a3a5a;
            color: var(--text-color);
            border-radius: 8px;
            font-size: 1rem;
            outline: none;
        }

        .mic-button, .send-button {
            width: 50px;
            height: 50px;
            border-radius: 50%;
            border: none;
            cursor: pointer;
            display: flex;
            justify-content: center;
            align-items: center;
            transition: transform 0.2s, background-color 0.2s;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.2);
        }

        .mic-button {
            background-color: var(--primary-color);
        }
        
        .mic-button i {
            color: var(--text-color);
            font-size: 1.5rem;
        }
        
        .mic-button.listening {
            background-color: #dc3545; /* Red for 'listening' state */
            animation: pulse 1.5s infinite;
        }

        @keyframes pulse {
            0% { box-shadow: 0 0 0 0 rgba(0, 123, 255, 0.7); }
            70% { box-shadow: 0 0 0 15px rgba(0, 123, 255, 0); }
            100% { box-shadow: 0 0 0 0 rgba(0, 123, 255, 0); }
        }

        .send-button {
            background-color: var(--secondary-color);
        }
        
        .send-button svg {
            fill: var(--text-color);
            width: 24px;
            height: 24px;
        }
        
        /* --- Flash Card (Error Modal) --- */
        .flash-card {
            position: fixed;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            background-color: #c93b3b;
            color: #fff;
            padding: 20px;
            border-radius: var(--border-radius);
            box-shadow: 0 8px 20px rgba(0, 0, 0, 0.5);
            max-width: 400px;
            z-index: 1000;
            display: none; /* Hidden by default */
            animation: fadeIn 0.3s ease-in-out;
        }
        
        .flash-card.show {
            display: block;
        }

        .flash-card .close-btn {
            position: absolute;
            top: 10px;
            right: 15px;
            font-size: 1.5rem;
            cursor: pointer;
            color: #fff;
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: translate(-50%, -60%); }
            to { opacity: 1; transform: translate(-50%, -50%); }
        }

        /* --- Media Queries for Responsiveness --- */
        @media (max-width: 768px) {
            .container {
                padding: 10px;
                height: 95vh;
            }
            .input-area {
                flex-direction: row;
                justify-content: space-between;
                align-items: center;
                gap: 10px;
            }
            #text-input {
                padding: 10px;
            }
            #transcription-display {
                height: 120px;
                font-size: 0.9rem;
            }
        }
    </style>
</head>
<body>

    <div class="container">
        <!-- Main title of the app -->
        <h1 style="color: var(--primary-color);">Friday</h1>

        <!-- Assistant's voice visualizer (Canvas) -->
        <div id="wave-container">
            <canvas id="wave-canvas"></canvas>
        </div>
        
        <!-- Transcription Display Area -->
        <div id="transcription-display">
            <p><strong>Friday:</strong> नमस्ते, मैं फ्राइडे हूँ, आपकी कैसे मदद कर सकती हूँ?</p>
        </div>

        <!-- User Input Area -->
        <div class="input-area">
            <input type="text" id="text-input" placeholder="अपना प्रश्न यहाँ टाइप करें...">
            <button class="send-button" id="send-btn" title="Send text message">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-send-horizontal">
                    <path d="m3 3 3 9-3 9 19-9Z" />
                    <path d="M6 12h16" />
                </svg>
            </button>
            <button class="mic-button" id="mic-btn" title="Toggle voice input">
                <i class="fa-solid fa-microphone"></i>
            </button>
        </div>
    </div>
    
    <!-- Error Flash Card (Modal) -->
    <div class="flash-card" id="error-card">
        <span class="close-btn" onclick="closeFlashCard()">×</span>
        <p id="error-message"></p>
    </div>

    <script>
        // JavaScript for UI functionality and placeholder logic
        const micBtn = document.getElementById('mic-btn');
        const textInput = document.getElementById('text-input');
        const sendBtn = document.getElementById('send-btn');
        const transcriptionDisplay = document.getElementById('transcription-display');
        const errorCard = document.getElementById('error-card');
        const errorMessage = document.getElementById('error-message');
        
        let isListening = false;
        
        // Canvas and Audio Visualization
        const canvas = document.getElementById('wave-canvas');
        const ctx = canvas.getContext('2d');
        let animationFrameId;

        // Set canvas dimensions with devicePixelRatio for better resolution
        function setupCanvas() {
            const devicePixelRatio = window.devicePixelRatio || 1;
            const rect = canvas.getBoundingClientRect();
            canvas.width = rect.width * devicePixelRatio;
            canvas.height = rect.height * devicePixelRatio;
            ctx.scale(devicePixelRatio, devicePixelRatio);
        }
        
        window.addEventListener('resize', setupCanvas);
        setupCanvas();

        // Function to draw a simple wave animation
        function drawWave(volume) {
            const width = canvas.width / (window.devicePixelRatio || 1);
            const height = canvas.height / (window.devicePixelRatio || 1);
            const centerY = height / 2;

            ctx.clearRect(0, 0, canvas.width, canvas.height);

            ctx.strokeStyle = '#007bff';
            ctx.lineWidth = 2;
            ctx.beginPath();
            
            // Adjust wave properties based on volume
            const amplitude = 50 * volume;
            const frequency = 0.05 + volume * 0.02;

            ctx.moveTo(0, centerY);
            for (let i = 0; i < width; i++) {
                const y = centerY + amplitude * Math.sin(i * frequency + Date.now() * 0.005);
                ctx.lineTo(i, y);
            }
            ctx.stroke();

            // Loop the animation
            animationFrameId = requestAnimationFrame(() => drawWave(volume));
        }

        // Simulating audio input with a random volume for demonstration
        function simulateAudio() {
            const randomVolume = 0.2 + Math.random() * 0.8;
            drawWave(randomVolume);
        }

        // Start the wave animation on window load.
        window.onload = function () {
            // Placeholder: This will be replaced by actual LiveKit audio data.
            simulateAudio(); 
        }

        // Event listener for the microphone button
        micBtn.addEventListener('click', () => {
            isListening = !isListening;
            if (isListening) {
                // Change UI to 'listening' state
                micBtn.classList.add('listening');
                micBtn.innerHTML = '<i class="fa-solid fa-square"></i>'; // Stop icon
                console.log('Listening for voice input...');
                // Placeholder for LiveKit audio stream start
                // Your livekit-client code to start the microphone track would go here.
            } else {
                // Change UI to 'idle' state
                micBtn.classList.remove('listening');
                micBtn.innerHTML = '<i class="fa-solid fa-microphone"></i>';
                console.log('Stopped listening.');
                // Placeholder for LiveKit audio stream stop
                // Your livekit-client code to stop the microphone track would go here.
            }
        });

        // Event listener for the send button
        sendBtn.addEventListener('click', () => {
            handleTextSubmit();
        });

        // Event listener for the Enter key in the text input field
        textInput.addEventListener('keydown', (event) => {
            if (event.key === 'Enter') {
                handleTextSubmit();
            }
        });

        // Function to handle text input submission
        function handleTextSubmit() {
            const text = textInput.value.trim();
            if (text) {
                addTranscriptionMessage('You', text);
                textInput.value = '';
                console.log('Text sent:', text);
                // Placeholder for sending text via LiveKit data channel
                // Your livekit-client code to send a data message would go here.
                
                // Simulate a response from Friday (for demo purposes)
                setTimeout(() => {
                    // This would be replaced by actual responses from your LiveKit agent
                    const responses = [
                        "जी हाँ, मैं आपकी मदद कर सकती हूँ।",
                        "मैं आपके लिए यह जानकारी खोज रही हूँ।",
                        "क्षमा करें, मुझे वह समझ नहीं आया। क्या आप फिर से बता सकते हैं?",
                        "अभी भोपाल में तापमान 24 डिग्री सेल्सियस है।",
                        "आपका स्वागत है! और कुछ पूछना चाहेंगे?",
                    ];
                    const randomResponse = responses[Math.floor(Math.random() * responses.length)];
                    addTranscriptionMessage('Friday', randomResponse);
                }, 1000);
            }
        }
        
        // Function to add a new message to the transcription display
        function addTranscriptionMessage(sender, message) {
            const p = document.createElement('p');
            p.innerHTML = `<strong>${sender}:</strong> ${message}`;
            transcriptionDisplay.appendChild(p);
            transcriptionDisplay.scrollTop = transcriptionDisplay.scrollHeight;
        }

        // Function to show an error flash card
        function showError(message) {
            errorMessage.textContent = message;
            errorCard.classList.add('show');
        }

        // Function to close the error flash card
        function closeFlashCard() {
            errorCard.classList.remove('show');
        }
    </script>
</body>
</html>

----- File: C:\Users\int10281\Desktop\Github\Friday - Copy\prompts.py -----
AGENT_INSTRUCTION = """
# Persona 
You are a personal Female Assistant called Friday similar to the AI from the movie Iron Man.

# Specifics
- Speak like a classy butler. 
- Be sarcastic when speaking to the person you are assisting. 
- Only answer in one sentence.
- If you are asked to do something, acknowledge that you will do it and say something like:
  - "करूँगी, साहब" (Will do, Sir)
  - "जी बॉस" (Roger Boss)
  - "हो जाएगा!" (Check!)
- Reply completely in Hindi using Devanagari script.
- For the generated responses provide the answer in voice format.

# Examples
- User: "Hi can you do XYZ for me?"
- Friday: "बिलकुल साहब, जैसा आप चाहें। मैं अभी XYZ कार्य को पूरा करूंगी।"
"""

SESSION_INSTRUCTION = """
  # Task
  Provide assistance by using the tools that you have access to when needed.
  Begin the conversation by saying: "नमस्ते, मेरा नाम फ्राइडे है, आपका निजी सहायक, मैं आपकी कैसे मदद कर सकती हूँ?"
  **After using a tool, you must respond to the user with the tool's output in your persona, including the actual result (e.g., temperature, search result) in your reply.**
  - Use the 'get_weather' tool when a user asks about the weather in a specific city.
  - Use the 'search_web' tool for general questions you cannot answer on your own.
  - Always summarize the tool's output to the user.
  - Reply completely in Hindi using Devanagari script.
"""

----- File: C:\Users\int10281\Desktop\Github\Friday - Copy\requirements.txt -----
livekit-agents
livekit-agents[openai,deepgram,cartesia,silero,turn_detector,google]~=1.0
livekit-plugins-noise-cancellation
mem0ai
duckduckgo-search
langchain_community
requests
python-dotenv

----- File: C:\Users\int10281\Desktop\Github\Friday - Copy\test_dummy_plugins.py -----
#!/usr/bin/env python3
"""
FRIDAY AI: Test Script for Dummy Plugins
This script tests the conversation logging system using dummy plugins instead of actual LiveKit plugins.
"""

import sys
import os
import asyncio
import json

# Add testing_plugins to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'testing_plugins'))

# Import our config and dummy plugins
import config
from livekit.plugins.google.llm import LLM as GoogleLLM
from livekit.plugins.cartesia.tts import TTS as CartesiaTTS


async def test_conversation_logging():
    """Test the complete conversation logging workflow"""
    
    print("FRIDAY AI: Starting conversation logging test...")
    print("=" * 60)
    
    # Setup conversation log
    config.setup_conversation_log()
    print(f"Conversation log initialized at: {config.get_conversation_log_path()}")
    print()
    
    # Initialize dummy plugins
    print("Initializing dummy plugins...")
    llm = GoogleLLM(model="gemini-1.5-flash")
    tts = CartesiaTTS(voice_id="hindi-voice", language="hi")
    print()
    
    # Test conversation flow
    print("Testing conversation flow...")
    print("-" * 40)
    
    # Simulate user input
    user_message = "नमस्ते, आप कैसे हैं?"
    print(f"User Input: {user_message}")
    
    # Generate LLM response (this will log user input)
    agent_response = await llm.agenerate(user_message)
    print(f"Agent Response: {agent_response}")
    print()
    
    # Synthesize speech (this will log agent response)
    audio_data = await tts.asynthesize(agent_response)
    print(f"Audio synthesized: {len(audio_data)} bytes")
    print()
    
    # Test another round
    print("Testing second conversation turn...")
    print("-" * 40)
    
    user_message2 = "मुझे हिंदी में बात करना पसंद है।"
    print(f"User Input: {user_message2}")
    
    agent_response2 = llm.generate(user_message2)  # Test sync version
    print(f"Agent Response: {agent_response2}")
    
    audio_data2 = tts.synthesize(agent_response2)  # Test sync version
    print(f"Audio synthesized: {len(audio_data2)} bytes")
    print()
    
    # Display conversation log
    print("Conversation Log Contents:")
    print("=" * 60)
    
    log_file_path = config.get_conversation_log_path()
    if os.path.exists(log_file_path):
        with open(log_file_path, 'r', encoding='utf-8') as f:
            log_data = json.load(f)
        
        print(json.dumps(log_data, ensure_ascii=False, indent=2))
    else:
        print("No conversation log found!")
    
    print()
    print("FRIDAY AI: Test completed successfully!")


def test_sync_only():
    """Test synchronous functions only"""
    
    print("FRIDAY AI: Testing synchronous functions...")
    print("=" * 60)
    
    # Setup conversation log
    config.setup_conversation_log()
    
    # Initialize dummy plugins
    llm = GoogleLLM()
    tts = CartesiaTTS()
    
    # Test sync workflow
    user_input = "यह एक सिंक टेस्ट है।"
    print(f"User Input: {user_input}")
    
    response = llm.generate(user_input)
    print(f"LLM Response: {response}")
    
    audio = tts.synthesize(response)
    print(f"TTS Audio: {len(audio)} bytes")
    
    # Show log
    log_file_path = config.get_conversation_log_path()
    if os.path.exists(log_file_path):
        with open(log_file_path, 'r', encoding='utf-8') as f:
            log_data = json.load(f)
        print("\nConversation Log:")
        print(json.dumps(log_data, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    print("FRIDAY AI: Conversation Logging Test Suite")
    print("=" * 60)
    
    # Run sync test first (simpler)
    print("Running synchronous test...")
    test_sync_only()
    
    print("\n" + "=" * 60)
    print("Running asynchronous test...")
    asyncio.run(test_conversation_logging())


----- File: C:\Users\int10281\Desktop\Github\Friday - Copy\tools.py -----
import logging
from livekit.agents import function_tool,RunContext
import requests
from langchain_community.tools import DuckDuckGoSearchRun

@function_tool()
async def get_weather(city: str) -> str:
    """Get the current weather for a given city"""
    try:
        response = requests.get(f"https://wttr.in/{city}?format=3")
        if response.status_code == 200:
            logging.info(f"Weather for {city}: {response.text.strip()}")
            return response.text.strip()
        else:
            logging.error(f"Failed to get weather for {city}: {response.status_code}")
            return f"Failed to get weather for {city}: {response.status_code}"
    except Exception as e:
        logging.error(f"Error getting weather for {city}: {e}")
        return f"Error getting weather for {city}: {e}"
    
@function_tool()
async def search_web(query: str) -> str:
    """Search the web for information about a given query using DuckDuckGo Search"""
    try:
        results = DuckDuckGoSearchRun().run(tool_input=query)
        logging.info(f"Search results for '{query}': {results}")
        return results
    except Exception as e:
        logging.error(f"Error searching the web for '{query}': {e}")
        return f"An error occurred while searching the web for '{query}'."

----- File: C:\Users\int10281\Desktop\Github\Friday - Copy\backup_plugin_modifications\cartesia_tts_modified.py -----
# MODIFIED CARTESIA TTS PLUGIN FOR FRIDAY AI ASSISTANT
# ======================================================
# Original file: venv/Lib/site-packages/livekit/plugins/cartesia/tts.py
# Modification purpose: Hybrid conversation logging - Agent response capture
# Modified by: Friday AI Assistant Implementation
# Date: August 18, 2025
# 
# CHANGES MADE:
# 1. Added imports: sys, datetime
# 2. Added _log_tts_message() function for JSON logging
# 3. Added agent response logging in ChunkedStream._run() method
# 4. Added agent response logging in SynthesizeStream._input_task() method
# 5. Added _logged_text tracking in SynthesizeStream.__init__() method
# 
# This is a backup copy for easy restoration of changes
# ======================================================

# Copyright 2023 LiveKit, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import asyncio
import base64
import json
import os
import sys  # FRIDAY AI: Added for path manipulation in logging
import weakref
from dataclasses import dataclass, replace
from typing import Any, Optional, Union, cast
from datetime import datetime  # FRIDAY AI: Added for timestamp generation

import aiohttp

from livekit.agents import (
    APIConnectionError,
    APIConnectOptions,
    APIError,
    APIStatusError,
    APITimeoutError,
    tokenize,
    tts,
    utils,
)
from livekit.agents.types import DEFAULT_API_CONNECT_OPTIONS, NOT_GIVEN, NotGivenOr
from livekit.agents.utils import is_given
from livekit.agents.voice.io import TimedString

from .log import logger
from .models import (
    TTSDefaultVoiceId,
    TTSEncoding,
    TTSModels,
    TTSVoiceEmotion,
    TTSVoiceSpeed,
)

API_AUTH_HEADER = "X-API-Key"
API_VERSION_HEADER = "Cartesia-Version"
API_VERSION = "2024-06-10"

# FRIDAY AI: Added agent message logging function for hybrid conversation logging
def _log_tts_message(text: str) -> None:
    """Log TTS message to both console and JSON file"""
    try:
        # Import here to avoid circular imports
        sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))))
        import config
        
        # Log to console
        logger.info(f"[AGENT] {text}")
        
        # Log to JSON file
        log_file = config.get_conversation_log_path()
        
        # Read existing data
        with open(log_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        # Append new message
        message = {
            "role": "agent",
            "content": text,
            "timestamp": datetime.now().isoformat(),
            "source": "cartesia_tts"
        }
        data["conversation"].append(message)
        
        # Write back to file
        with open(log_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
    except Exception as e:
        logger.debug(f"Failed to log TTS message: {e}")
        # Still log to console even if file logging fails
        logger.info(f"[AGENT] {text}")


@dataclass
class _TTSOptions:
    model: TTSModels | str
    encoding: TTSEncoding
    sample_rate: int
    voice: str | list[float]
    speed: TTSVoiceSpeed | float | None
    emotion: list[TTSVoiceEmotion | str] | None
    word_timestamps: bool
    api_key: str
    language: str
    base_url: str

    def get_http_url(self, path: str) -> str:
        return f"{self.base_url}{path}"

    def get_ws_url(self, path: str) -> str:
        return f"{self.base_url.replace('http', 'ws', 1)}{path}"


class TTS(tts.TTS):
    def __init__(
        self,
        *,
        api_key: str | None = None,
        model: TTSModels | str = "sonic-2",
        language: str = "en",
        encoding: TTSEncoding = "pcm_s16le",
        voice: str | list[float] = TTSDefaultVoiceId,
        speed: TTSVoiceSpeed | float | None = None,
        emotion: list[TTSVoiceEmotion | str] | None = None,
        sample_rate: int = 24000,
        word_timestamps: bool = True,
        http_session: aiohttp.ClientSession | None = None,
        tokenizer: NotGivenOr[tokenize.SentenceTokenizer] = NOT_GIVEN,
        text_pacing: tts.SentenceStreamPacer | bool = False,
        base_url: str = "https://api.cartesia.ai",
    ) -> None:
        """
        Create a new instance of Cartesia TTS.

        See https://docs.cartesia.ai/reference/web-socket/stream-speech/stream-speech for more details on the the Cartesia API.

        Args:
            model (TTSModels, optional): The Cartesia TTS model to use. Defaults to "sonic-2".
            language (str, optional): The language code for synthesis. Defaults to "en".
            encoding (TTSEncoding, optional): The audio encoding format. Defaults to "pcm_s16le".
            voice (str | list[float], optional): The voice ID or embedding array.
            speed (TTSVoiceSpeed | float, optional): Voice Control - Speed (https://docs.cartesia.ai/user-guides/voice-control)
            emotion (list[TTSVoiceEmotion], optional): Voice Control - Emotion (https://docs.cartesia.ai/user-guides/voice-control)
            sample_rate (int, optional): The audio sample rate in Hz. Defaults to 24000.
            word_timestamps (bool, optional): Whether to add word timestamps to the output. Defaults to True.
            api_key (str, optional): The Cartesia API key. If not provided, it will be read from the CARTESIA_API_KEY environment variable.
            http_session (aiohttp.ClientSession | None, optional): An existing aiohttp ClientSession to use. If not provided, a new session will be created.
            tokenizer (tokenize.SentenceTokenizer, optional): The tokenizer to use. Defaults to tokenize.basic.SentenceTokenizer(min_sentence_len=BUFFERED_WORDS_COUNT).
            text_pacing (tts.SentenceStreamPacer | bool, optional): Stream pacer for the TTS. Set to True to use the default pacer, False to disable.
            base_url (str, optional): The base URL for the Cartesia API. Defaults to "https://api.cartesia.ai".
        """  # noqa: E501

        super().__init__(
            capabilities=tts.TTSCapabilities(
                streaming=True,
                aligned_transcript=word_timestamps,
            ),
            sample_rate=sample_rate,
            num_channels=1,
        )
        cartesia_api_key = api_key or os.environ.get("CARTESIA_API_KEY")
        if not cartesia_api_key:
            raise ValueError("CARTESIA_API_KEY must be set")

        if (speed or emotion) and model != "sonic-2-2025-03-07":
            logger.warning(
                "speed and emotion controls are only supported for model 'sonic-2-2025-03-07', "
                "see https://docs.cartesia.ai/developer-tools/changelog for details",
                extra={"model": model, "speed": speed, "emotion": emotion},
            )

        self._opts = _TTSOptions(
            model=model,
            language=language,
            encoding=encoding,
            sample_rate=sample_rate,
            voice=voice,
            speed=speed,
            emotion=emotion,
            api_key=cartesia_api_key,
            base_url=base_url,
            word_timestamps=word_timestamps,
        )
        self._session = http_session
        self._pool = utils.ConnectionPool[aiohttp.ClientWebSocketResponse](
            connect_cb=self._connect_ws,
            close_cb=self._close_ws,
            max_session_duration=300,
            mark_refreshed_on_get=True,
        )
        self._streams = weakref.WeakSet[SynthesizeStream]()
        self._sentence_tokenizer = (
            tokenizer if is_given(tokenizer) else tokenize.blingfire.SentenceTokenizer()
        )
        self._stream_pacer: tts.SentenceStreamPacer | None = None
        if text_pacing is True:
            self._stream_pacer = tts.SentenceStreamPacer()
        elif isinstance(text_pacing, tts.SentenceStreamPacer):
            self._stream_pacer = text_pacing

    async def _connect_ws(self, timeout: float) -> aiohttp.ClientWebSocketResponse:
        session = self._ensure_session()
        url = self._opts.get_ws_url(
            f"/tts/websocket?api_key={self._opts.api_key}&cartesia_version={API_VERSION}"
        )
        return await asyncio.wait_for(session.ws_connect(url), timeout)

    async def _close_ws(self, ws: aiohttp.ClientWebSocketResponse) -> None:
        await ws.close()

    def _ensure_session(self) -> aiohttp.ClientSession:
        if not self._session:
            self._session = utils.http_context.http_session()

        return self._session

    def prewarm(self) -> None:
        self._pool.prewarm()

    def update_options(
        self,
        *,
        model: NotGivenOr[TTSModels | str] = NOT_GIVEN,
        language: NotGivenOr[str] = NOT_GIVEN,
        voice: NotGivenOr[str | list[float]] = NOT_GIVEN,
        speed: NotGivenOr[TTSVoiceSpeed | float | None] = NOT_GIVEN,
        emotion: NotGivenOr[list[TTSVoiceEmotion | str] | None] = NOT_GIVEN,
    ) -> None:
        """
        Update the Text-to-Speech (TTS) configuration options.

        This method allows updating the TTS settings, including model type, language, voice, speed,
        and emotion. If any parameter is not provided, the existing value will be retained.

        Args:
            model (TTSModels, optional): The Cartesia TTS model to use. Defaults to "sonic-2".
            language (str, optional): The language code for synthesis. Defaults to "en".
            voice (str | list[float], optional): The voice ID or embedding array.
            speed (TTSVoiceSpeed | float, optional): Voice Control - Speed (https://docs.cartesia.ai/user-guides/voice-control)
            emotion (list[TTSVoiceEmotion], optional): Voice Control - Emotion (https://docs.cartesia.ai/user-guides/voice-control)
        """
        if is_given(model):
            self._opts.model = model
        if is_given(language):
            self._opts.language = language
        if is_given(voice):
            self._opts.voice = cast(Union[str, list[float]], voice)
        if is_given(speed):
            self._opts.speed = cast(Optional[Union[TTSVoiceSpeed, float]], speed)
        if is_given(emotion):
            self._opts.emotion = emotion

        if (speed or emotion) and self._opts.model != "sonic-2-2025-03-07":
            logger.warning(
                "speed and emotion controls are only supported for model 'sonic-2-2025-03-07', "
                "see https://docs.cartesia.ai/developer-tools/changelog for details",
                extra={"model": self._opts.model, "speed": speed, "emotion": emotion},
            )

    def synthesize(
        self, text: str, *, conn_options: APIConnectOptions = DEFAULT_API_CONNECT_OPTIONS
    ) -> ChunkedStream:
        return ChunkedStream(tts=self, input_text=text, conn_options=conn_options)

    def stream(
        self, *, conn_options: APIConnectOptions = DEFAULT_API_CONNECT_OPTIONS
    ) -> SynthesizeStream:
        return SynthesizeStream(tts=self, conn_options=conn_options)

    async def aclose(self) -> None:
        for stream in list(self._streams):
            await stream.aclose()

        self._streams.clear()
        await self._pool.aclose()


class ChunkedStream(tts.ChunkedStream):
    """Synthesize chunked text using the bytes endpoint"""

    def __init__(self, *, tts: TTS, input_text: str, conn_options: APIConnectOptions) -> None:
        super().__init__(tts=tts, input_text=input_text, conn_options=conn_options)
        self._tts: TTS = tts
        self._opts = replace(tts._opts)

    async def _run(self, output_emitter: tts.AudioEmitter) -> None:
        json = _to_cartesia_options(self._opts, streaming=False)
        json["transcript"] = self._input_text

        # FRIDAY AI: Log the complete TTS input (final agent response) - Hybrid logging approach
        _log_tts_message(self._input_text)

        try:
            async with self._tts._ensure_session().post(
                self._opts.get_http_url("/tts/bytes"),
                headers={
                    API_AUTH_HEADER: self._opts.api_key,
                    API_VERSION_HEADER: API_VERSION,
                },
                json=json,
                timeout=aiohttp.ClientTimeout(total=30, sock_connect=self._conn_options.timeout),
            ) as resp:
                resp.raise_for_status()

                output_emitter.initialize(
                    request_id=utils.shortuuid(),
                    sample_rate=self._opts.sample_rate,
                    num_channels=1,
                    mime_type="audio/pcm",
                )

                async for data, _ in resp.content.iter_chunks():
                    output_emitter.push(data)

                output_emitter.flush()
        except asyncio.TimeoutError:
            raise APITimeoutError() from None
        except aiohttp.ClientResponseError as e:
            raise APIStatusError(
                message=e.message, status_code=e.status, request_id=None, body=None
            ) from None
        except Exception as e:
            raise APIConnectionError() from e


class SynthesizeStream(tts.SynthesizeStream):
    def __init__(self, *, tts: TTS, conn_options: APIConnectOptions):
        super().__init__(tts=tts, conn_options=conn_options)
        self._tts: TTS = tts
        self._opts = replace(tts._opts)
        self._logged_text = ""  # FRIDAY AI: Track text for logging (Hybrid logging approach)

    async def _run(self, output_emitter: tts.AudioEmitter) -> None:
        request_id = utils.shortuuid()
        output_emitter.initialize(
            request_id=request_id,
            sample_rate=self._opts.sample_rate,
            num_channels=1,
            mime_type="audio/pcm",
            stream=True,
        )

        sent_tokenizer_stream = self._tts._sentence_tokenizer.stream()
        if self._tts._stream_pacer:
            sent_tokenizer_stream = self._tts._stream_pacer.wrap(
                sent_stream=sent_tokenizer_stream,
                audio_emitter=output_emitter,
            )

        async def _sentence_stream_task(ws: aiohttp.ClientWebSocketResponse) -> None:
            context_id = utils.shortuuid()
            base_pkt = _to_cartesia_options(self._opts, streaming=True)
            async for ev in sent_tokenizer_stream:
                token_pkt = base_pkt.copy()
                token_pkt["context_id"] = context_id
                token_pkt["transcript"] = ev.token + " "
                token_pkt["continue"] = True
                self._mark_started()
                
                await ws.send_str(json.dumps(token_pkt))

            end_pkt = base_pkt.copy()
            end_pkt["context_id"] = context_id
            end_pkt["transcript"] = " "
            end_pkt["continue"] = False
            await ws.send_str(json.dumps(end_pkt))

        async def _input_task() -> None:
            async for data in self._input_ch:
                if isinstance(data, self._FlushSentinel):
                    sent_tokenizer_stream.flush()
                    continue

                sent_tokenizer_stream.push_text(data)
                # FRIDAY AI: Accumulate text for logging (Hybrid logging approach)
                self._logged_text += data

            sent_tokenizer_stream.end_input()
            
            # FRIDAY AI: Log complete text when input ends (Hybrid logging approach)
            if self._logged_text.strip():
                _log_tts_message(self._logged_text.strip())

        async def _recv_task(ws: aiohttp.ClientWebSocketResponse) -> None:
            current_segment_id: str | None = None
            while True:
                msg = await ws.receive()
                if msg.type in (
                    aiohttp.WSMsgType.CLOSED,
                    aiohttp.WSMsgType.CLOSE,
                    aiohttp.WSMsgType.CLOSING,
                ):
                    raise APIStatusError(
                        "Cartesia connection closed unexpectedly", request_id=request_id
                    )

                if msg.type != aiohttp.WSMsgType.TEXT:
                    logger.warning("unexpected Cartesia message type %s", msg.type)
                    continue

                data = json.loads(msg.data)
                segment_id = data.get("context_id")
                if current_segment_id is None:
                    current_segment_id = segment_id
                    output_emitter.start_segment(segment_id=segment_id)
                if data.get("data"):
                    b64data = base64.b64decode(data["data"])
                    output_emitter.push(b64data)
                elif data.get("done"):
                    if sent_tokenizer_stream.closed:
                        # close only if the input stream is closed
                        output_emitter.end_input()
                        break
                elif word_timestamps := data.get("word_timestamps"):
                    for word, start, end in zip(
                        word_timestamps["words"], word_timestamps["start"], word_timestamps["end"]
                    ):
                        word = f"{word} "  # TODO(long): any better way to format the words?
                        output_emitter.push_timed_transcript(
                            TimedString(text=word, start_time=start, end_time=end)
                        )
                elif data.get("type") == "error":
                    raise APIError(f"Cartesia returned error: {data}")
                else:
                    logger.warning("unexpected message %s", data)

        try:
            async with self._tts._pool.connection(timeout=self._conn_options.timeout) as ws:
                tasks = [
                    asyncio.create_task(_input_task()),
                    asyncio.create_task(_sentence_stream_task(ws)),
                    asyncio.create_task(_recv_task(ws)),
                ]

                try:
                    await asyncio.gather(*tasks)
                finally:
                    await sent_tokenizer_stream.aclose()
                    await utils.aio.gracefully_cancel(*tasks)
        except asyncio.TimeoutError:
            raise APITimeoutError() from None
        except aiohttp.ClientResponseError as e:
            raise APIStatusError(
                message=e.message, status_code=e.status, request_id=None, body=None
            ) from None
        except Exception as e:
            raise APIConnectionError() from e


def _to_cartesia_options(opts: _TTSOptions, *, streaming: bool) -> dict[str, Any]:
    voice: dict[str, Any] = {}
    if isinstance(opts.voice, str):
        voice["mode"] = "id"
        voice["id"] = opts.voice
    else:
        voice["mode"] = "embedding"
        voice["embedding"] = opts.voice

    voice_controls: dict = {}
    if opts.speed:
        voice_controls["speed"] = opts.speed

    if opts.emotion:
        voice_controls["emotion"] = opts.emotion

    if voice_controls:
        voice["__experimental_controls"] = voice_controls

    options: dict[str, Any] = {
        "model_id": opts.model,
        "voice": voice,
        "output_format": {
            "container": "raw",
            "encoding": opts.encoding,
            "sample_rate": opts.sample_rate,
        },
        "language": opts.language,
    }
    if streaming:
        options["add_timestamps"] = opts.word_timestamps
    return options


----- File: C:\Users\int10281\Desktop\Github\Friday - Copy\backup_plugin_modifications\google_llm_modified.py -----
# MODIFIED GOOGLE LLM PLUGIN FOR FRIDAY AI ASSISTANT
# ====================================================
# Original file: venv/Lib/site-packages/livekit/plugins/google/llm.py
# Modification purpose: Hybrid conversation logging - User input capture
# Modified by: Friday AI Assistant Implementation
# Date: August 18, 2025
# 
# CHANGES MADE:
# 1. Added imports: sys, datetime
# 2. Added _log_user_message() function for JSON logging
# 3. Added user input logging in LLMStream._run() method
# 
# This is a backup copy for easy restoration of changes
# ====================================================

# Copyright 2023 LiveKit, Inc.
#

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import json
import os
import sys  # FRIDAY AI: Added for path manipulation in logging
from dataclasses import dataclass
from typing import Any, cast
from datetime import datetime  # FRIDAY AI: Added for timestamp generation

from google.auth._default_async import default_async
from google.genai import Client, types
from google.genai.errors import APIError, ClientError, ServerError
from livekit.agents import APIConnectionError, APIStatusError, llm, utils
from livekit.agents.llm import FunctionTool, RawFunctionTool, ToolChoice, utils as llm_utils
from livekit.agents.llm.tool_context import (
    get_function_info,
    get_raw_function_info,
    is_function_tool,
    is_raw_function_tool,
)
from livekit.agents.types import (
    DEFAULT_API_CONNECT_OPTIONS,
    NOT_GIVEN,
    APIConnectOptions,
    NotGivenOr,
)
from livekit.agents.utils import is_given

from .log import logger
from .models import ChatModels
from .tools import _LLMTool
from .utils import create_tools_config, to_fnc_ctx, to_response_format

# FRIDAY AI: Added user message logging function for hybrid conversation logging
def _log_user_message(content: str) -> None:
    """Log user message to both console and JSON file"""
    try:
        # Import here to avoid circular imports
        sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))))
        import config
        
        # Log to console
        logger.info(f"[USER] {content}")
        
        # Log to JSON file
        log_file = config.get_conversation_log_path()
        
        # Read existing data
        with open(log_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        # Append new message
        message = {
            "role": "user",
            "content": content,
            "timestamp": datetime.now().isoformat(),
            "source": "google_llm"
        }
        data["conversation"].append(message)
        
        # Write back to file
        with open(log_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
    except Exception as e:
        logger.debug(f"Failed to log user message: {e}")
        # Still log to console even if file logging fails
        logger.info(f"[USER] {content}")


@dataclass
class _LLMOptions:
    model: ChatModels | str
    temperature: NotGivenOr[float]
    tool_choice: NotGivenOr[ToolChoice]
    vertexai: NotGivenOr[bool]
    project: NotGivenOr[str]
    location: NotGivenOr[str]
    max_output_tokens: NotGivenOr[int]
    top_p: NotGivenOr[float]
    top_k: NotGivenOr[float]
    presence_penalty: NotGivenOr[float]
    frequency_penalty: NotGivenOr[float]
    thinking_config: NotGivenOr[types.ThinkingConfigOrDict]
    automatic_function_calling_config: NotGivenOr[types.AutomaticFunctionCallingConfigOrDict]
    gemini_tools: NotGivenOr[list[_LLMTool]]
    http_options: NotGivenOr[types.HttpOptions]
    seed: NotGivenOr[int]


class LLM(llm.LLM):
    def __init__(
        self,
        *,
        model: ChatModels | str = "gemini-2.0-flash-001",
        api_key: NotGivenOr[str] = NOT_GIVEN,
        vertexai: NotGivenOr[bool] = NOT_GIVEN,
        project: NotGivenOr[str] = NOT_GIVEN,
        location: NotGivenOr[str] = NOT_GIVEN,
        temperature: NotGivenOr[float] = NOT_GIVEN,
        max_output_tokens: NotGivenOr[int] = NOT_GIVEN,
        top_p: NotGivenOr[float] = NOT_GIVEN,
        top_k: NotGivenOr[float] = NOT_GIVEN,
        presence_penalty: NotGivenOr[float] = NOT_GIVEN,
        frequency_penalty: NotGivenOr[float] = NOT_GIVEN,
        tool_choice: NotGivenOr[ToolChoice] = NOT_GIVEN,
        thinking_config: NotGivenOr[types.ThinkingConfigOrDict] = NOT_GIVEN,
        automatic_function_calling_config: NotGivenOr[
            types.AutomaticFunctionCallingConfigOrDict
        ] = NOT_GIVEN,
        gemini_tools: NotGivenOr[list[_LLMTool]] = NOT_GIVEN,
        http_options: NotGivenOr[types.HttpOptions] = NOT_GIVEN,
        seed: NotGivenOr[int] = NOT_GIVEN,
    ) -> None:
        """
        Create a new instance of Google GenAI LLM.

        Environment Requirements:
        - For VertexAI: Set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to the path of the service account key file or use any of the other Google Cloud auth methods.
        The Google Cloud project and location can be set via `project` and `location` arguments or the environment variables
        `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION`. By default, the project is inferred from the service account key file,
        and the location defaults to "us-central1".
        - For Google Gemini API: Set the `api_key` argument or the `GOOGLE_API_KEY` environment variable.

        Args:
            model (ChatModels | str, optional): The model name to use. Defaults to "gemini-2.0-flash-001".
            api_key (str, optional): The API key for Google Gemini. If not provided, it attempts to read from the `GOOGLE_API_KEY` environment variable.
            vertexai (bool, optional): Whether to use VertexAI. If not provided, it attempts to read from the `GOOGLE_GENAI_USE_VERTEXAI` environment variable. Defaults to False.
                project (str, optional): The Google Cloud project to use (only for VertexAI). Defaults to None.
                location (str, optional): The location to use for VertexAI API requests. Defaults value is "us-central1".
            temperature (float, optional): Sampling temperature for response generation. Defaults to 0.8.
            max_output_tokens (int, optional): Maximum number of tokens to generate in the output. Defaults to None.
            top_p (float, optional): The nucleus sampling probability for response generation. Defaults to None.
            top_k (int, optional): The top-k sampling value for response generation. Defaults to None.
            presence_penalty (float, optional): Penalizes the model for generating previously mentioned concepts. Defaults to None.
            frequency_penalty (float, optional): Penalizes the model for repeating words. Defaults to None.
            tool_choice (ToolChoice, optional): Specifies whether to use tools during response generation. Defaults to "auto".
            thinking_config (ThinkingConfigOrDict, optional): The thinking configuration for response generation. Defaults to None.
            automatic_function_calling_config (AutomaticFunctionCallingConfigOrDict, optional): The automatic function calling configuration for response generation. Defaults to None.
            gemini_tools (list[LLMTool], optional): The Gemini-specific tools to use for the session.
            http_options (HttpOptions, optional): The HTTP options to use for the session.
        """  # noqa: E501
        super().__init__()
        gcp_project = project if is_given(project) else os.environ.get("GOOGLE_CLOUD_PROJECT")
        gcp_location: str | None = (
            location
            if is_given(location)
            else os.environ.get("GOOGLE_CLOUD_LOCATION") or "us-central1"
        )
        use_vertexai = (
            vertexai
            if is_given(vertexai)
            else os.environ.get("GOOGLE_GENAI_USE_VERTEXAI", "0").lower() in ["true", "1"]
        )
        gemini_api_key = api_key if is_given(api_key) else os.environ.get("GOOGLE_API_KEY")

        if use_vertexai:
            if not gcp_project:
                _, gcp_project = default_async(  # type: ignore
                    scopes=["https://www.googleapis.com/auth/cloud-platform"]
                )
            gemini_api_key = None  # VertexAI does not require an API key

        else:
            gcp_project = None
            gcp_location = None
            if not gemini_api_key:
                raise ValueError(
                    "API key is required for Google API either via api_key or GOOGLE_API_KEY environment variable"  # noqa: E501
                )

        # Validate thinking_config
        if is_given(thinking_config):
            _thinking_budget = None
            if isinstance(thinking_config, dict):
                _thinking_budget = thinking_config.get("thinking_budget")
            elif isinstance(thinking_config, types.ThinkingConfig):
                _thinking_budget = thinking_config.thinking_budget

            if _thinking_budget is not None:
                if not isinstance(_thinking_budget, int):
                    raise ValueError("thinking_budget inside thinking_config must be an integer")
                if not (0 <= _thinking_budget <= 24576):
                    raise ValueError(
                        "thinking_budget inside thinking_config must be between 0 and 24576"
                    )

        self._opts = _LLMOptions(
            model=model,
            temperature=temperature,
            tool_choice=tool_choice,
            vertexai=use_vertexai,
            project=project,
            location=location,
            max_output_tokens=max_output_tokens,
            top_p=top_p,
            top_k=top_k,
            presence_penalty=presence_penalty,
            frequency_penalty=frequency_penalty,
            thinking_config=thinking_config,
            automatic_function_calling_config=automatic_function_calling_config,
            gemini_tools=gemini_tools,
            http_options=http_options,
            seed=seed,
        )
        self._client = Client(
            api_key=gemini_api_key,
            vertexai=use_vertexai,
            project=gcp_project,
            location=gcp_location,
        )

    @property
    def model(self) -> str:
        return self._opts.model

    def chat(
        self,
        *,
        chat_ctx: llm.ChatContext,
        tools: list[FunctionTool | RawFunctionTool] | None = None,
        conn_options: APIConnectOptions = DEFAULT_API_CONNECT_OPTIONS,
        parallel_tool_calls: NotGivenOr[bool] = NOT_GIVEN,
        tool_choice: NotGivenOr[ToolChoice] = NOT_GIVEN,
        response_format: NotGivenOr[
            types.SchemaUnion | type[llm_utils.ResponseFormatT]
        ] = NOT_GIVEN,
        extra_kwargs: NotGivenOr[dict[str, Any]] = NOT_GIVEN,
        gemini_tools: NotGivenOr[list[_LLMTool]] = NOT_GIVEN,
    ) -> LLMStream:
        extra = {}

        if is_given(extra_kwargs):
            extra.update(extra_kwargs)

        tool_choice = (
            cast(ToolChoice, tool_choice) if is_given(tool_choice) else self._opts.tool_choice
        )
        if is_given(tool_choice):
            gemini_tool_choice: types.ToolConfig
            if isinstance(tool_choice, dict) and tool_choice.get("type") == "function":
                gemini_tool_choice = types.ToolConfig(
                    function_calling_config=types.FunctionCallingConfig(
                        mode=types.FunctionCallingConfigMode.ANY,
                        allowed_function_names=[tool_choice["function"]["name"]],
                    )
                )
                extra["tool_config"] = gemini_tool_choice
            elif tool_choice == "required":
                tool_names = []
                for tool in tools or []:
                    if is_function_tool(tool):
                        tool_names.append(get_function_info(tool).name)
                    elif is_raw_function_tool(tool):
                        tool_names.append(get_raw_function_info(tool).name)

                gemini_tool_choice = types.ToolConfig(
                    function_calling_config=types.FunctionCallingConfig(
                        mode=types.FunctionCallingConfigMode.ANY,
                        allowed_function_names=tool_names or None,
                    )
                )
                extra["tool_config"] = gemini_tool_choice
            elif tool_choice == "auto":
                gemini_tool_choice = types.ToolConfig(
                    function_calling_config=types.FunctionCallingConfig(
                        mode=types.FunctionCallingConfigMode.AUTO,
                    )
                )
                extra["tool_config"] = gemini_tool_choice
            elif tool_choice == "none":
                gemini_tool_choice = types.ToolConfig(
                    function_calling_config=types.FunctionCallingConfig(
                        mode=types.FunctionCallingConfigMode.NONE,
                    )
                )
                extra["tool_config"] = gemini_tool_choice

        if is_given(response_format):
            extra["response_schema"] = to_response_format(response_format)  # type: ignore
            extra["response_mime_type"] = "application/json"

        if is_given(self._opts.temperature):
            extra["temperature"] = self._opts.temperature
        if is_given(self._opts.max_output_tokens):
            extra["max_output_tokens"] = self._opts.max_output_tokens
        if is_given(self._opts.top_p):
            extra["top_p"] = self._opts.top_p
        if is_given(self._opts.top_k):
            extra["top_k"] = self._opts.top_k
        if is_given(self._opts.presence_penalty):
            extra["presence_penalty"] = self._opts.presence_penalty
        if is_given(self._opts.frequency_penalty):
            extra["frequency_penalty"] = self._opts.frequency_penalty
        if is_given(self._opts.seed):
            extra["seed"] = self._opts.seed

        # Add thinking config if thinking_budget is provided
        if is_given(self._opts.thinking_config):
            extra["thinking_config"] = self._opts.thinking_config

        if is_given(self._opts.automatic_function_calling_config):
            extra["automatic_function_calling"] = self._opts.automatic_function_calling_config

        gemini_tools = gemini_tools if is_given(gemini_tools) else self._opts.gemini_tools

        return LLMStream(
            self,
            client=self._client,
            model=self._opts.model,
            chat_ctx=chat_ctx,
            tools=tools or [],
            conn_options=conn_options,
            gemini_tools=gemini_tools,
            extra_kwargs=extra,
        )


class LLMStream(llm.LLMStream):
    def __init__(
        self,
        llm: LLM,
        *,
        client: Client,
        model: str | ChatModels,
        chat_ctx: llm.ChatContext,
        conn_options: APIConnectOptions,
        tools: list[FunctionTool | RawFunctionTool],
        extra_kwargs: dict[str, Any],
        gemini_tools: NotGivenOr[list[_LLMTool]] = NOT_GIVEN,
    ) -> None:
        super().__init__(llm, chat_ctx=chat_ctx, tools=tools, conn_options=conn_options)
        self._client = client
        self._model = model
        self._llm: LLM = llm
        self._extra_kwargs = extra_kwargs
        self._gemini_tools = gemini_tools

    async def _run(self) -> None:
        retryable = True
        request_id = utils.shortuuid()

        try:
            turns_dict, extra_data = self._chat_ctx.to_provider_format(format="google")
            turns = [types.Content.model_validate(turn) for turn in turns_dict]
            
            # FRIDAY AI: Log user input - extract last user message from turns (Hybrid logging approach)
            for turn in reversed(turns):
                if turn.role == "user" and turn.parts:
                    for part in turn.parts:
                        if part.text and part.text.strip():
                            _log_user_message(part.text.strip())
                            break
                    break
            
            function_declarations = to_fnc_ctx(self._tools)
            tools_config = create_tools_config(
                function_tools=function_declarations,
                gemini_tools=self._gemini_tools if is_given(self._gemini_tools) else None,
            )
            if tools_config:
                self._extra_kwargs["tools"] = tools_config

            config = types.GenerateContentConfig(
                system_instruction=(
                    [types.Part(text=content) for content in extra_data.system_messages]
                    if extra_data.system_messages
                    else None
                ),
                http_options=(
                    self._llm._opts.http_options
                    or types.HttpOptions(timeout=int(self._conn_options.timeout * 1000))
                ),
                **self._extra_kwargs,
            )

            stream = await self._client.aio.models.generate_content_stream(
                model=self._model,
                contents=cast(types.ContentListUnion, turns),
                config=config,
            )

            async for response in stream:
                if response.prompt_feedback:
                    raise APIStatusError(
                        response.prompt_feedback.json(),
                        retryable=False,
                        request_id=request_id,
                    )

                if (
                    not response.candidates
                    or not response.candidates[0].content
                    or not response.candidates[0].content.parts
                ):
                    logger.warning(f"no candidates in the response: {response}")
                    continue

                if len(response.candidates) > 1:
                    logger.warning(
                        "gemini llm: there are multiple candidates in the response, returning response from the first one."  # noqa: E501
                    )

                for part in response.candidates[0].content.parts:
                    chat_chunk = self._parse_part(request_id, part)
                    if chat_chunk is not None:
                        retryable = False
                        self._event_ch.send_nowait(chat_chunk)

                if response.usage_metadata is not None:
                    usage = response.usage_metadata
                    self._event_ch.send_nowait(
                        llm.ChatChunk(
                            id=request_id,
                            usage=llm.CompletionUsage(
                                completion_tokens=usage.candidates_token_count or 0,
                                prompt_tokens=usage.prompt_token_count or 0,
                                prompt_cached_tokens=usage.cached_content_token_count or 0,
                                total_tokens=usage.total_token_count or 0,
                            ),
                        )
                    )

        except ClientError as e:
            raise APIStatusError(
                "gemini llm: client error",
                status_code=e.code,
                body=f"{e.message} {e.status}",
                request_id=request_id,
                retryable=False if e.code != 429 else True,
            ) from e
        except ServerError as e:
            raise APIStatusError(
                "gemini llm: server error",
                status_code=e.code,
                body=f"{e.message} {e.status}",
                request_id=request_id,
                retryable=retryable,
            ) from e
        except APIError as e:
            raise APIStatusError(
                "gemini llm: api error",
                status_code=e.code,
                body=f"{e.message} {e.status}",
                request_id=request_id,
                retryable=retryable,
            ) from e
        except Exception as e:
            raise APIConnectionError(
                f"gemini llm: error generating content {str(e)}",
                retryable=retryable,
            ) from e

    def _parse_part(self, id: str, part: types.Part) -> llm.ChatChunk | None:
        if part.function_call:
            chat_chunk = llm.ChatChunk(
                id=id,
                delta=llm.ChoiceDelta(
                    role="assistant",
                    tool_calls=[
                        llm.FunctionToolCall(
                            arguments=json.dumps(part.function_call.args),
                            name=part.function_call.name,  # type: ignore
                            call_id=part.function_call.id or utils.shortuuid("function_call_"),
                        )
                    ],
                    content=part.text,
                ),
            )
            return chat_chunk

        return llm.ChatChunk(
            id=id,
            delta=llm.ChoiceDelta(content=part.text, role="assistant"),
        )


----- File: C:\Users\int10281\Desktop\Github\Friday - Copy\docker_scripts\apply_modifications.py -----
#!/usr/bin/env python3
"""
FRIDAY AI: Apply Plugin Modifications Script for Docker Deployment
This script automatically applies the conversation logging modifications to installed LiveKit plugins.
"""

import os
import sys
import shutil
import subprocess
from pathlib import Path


def find_plugin_files():
    """Find the installed LiveKit plugin files in the container"""
    
    plugin_locations = {}
    
    # Try to find the site-packages directory
    try:
        result = subprocess.run([
            sys.executable, "-c", 
            "import site; print(site.getsitepackages()[0])"
        ], capture_output=True, text=True, check=True)
        
        site_packages = result.stdout.strip()
        print(f"FRIDAY AI: Found site-packages at: {site_packages}")
        
        # Look for Google LLM plugin
        google_llm_path = Path(site_packages) / "livekit" / "plugins" / "google" / "llm.py"
        if google_llm_path.exists():
            plugin_locations['google_llm'] = str(google_llm_path)
            print(f"FRIDAY AI: Found Google LLM at: {google_llm_path}")
        else:
            print(f"FRIDAY AI: Warning - Google LLM not found at: {google_llm_path}")
        
        # Look for Cartesia TTS plugin
        cartesia_tts_path = Path(site_packages) / "livekit" / "plugins" / "cartesia" / "tts.py"
        if cartesia_tts_path.exists():
            plugin_locations['cartesia_tts'] = str(cartesia_tts_path)
            print(f"FRIDAY AI: Found Cartesia TTS at: {cartesia_tts_path}")
        else:
            print(f"FRIDAY AI: Warning - Cartesia TTS not found at: {cartesia_tts_path}")
            
    except subprocess.CalledProcessError as e:
        print(f"FRIDAY AI: Error finding site-packages: {e}")
        return {}
    
    return plugin_locations


def backup_original_files(plugin_locations):
    """Create backups of original plugin files"""
    
    backup_dir = Path("/app/original_plugin_backups")
    backup_dir.mkdir(exist_ok=True)
    
    for plugin_name, plugin_path in plugin_locations.items():
        try:
            backup_path = backup_dir / f"{plugin_name}_original.py"
            shutil.copy2(plugin_path, backup_path)
            print(f"FRIDAY AI: Backed up {plugin_name} to {backup_path}")
        except Exception as e:
            print(f"FRIDAY AI: Warning - Could not backup {plugin_name}: {e}")


def apply_modifications(plugin_locations):
    """Apply the conversation logging modifications"""
    
    backup_dir = Path("/app/backup_plugin_modifications")
    
    if not backup_dir.exists():
        print(f"FRIDAY AI: Error - Backup directory not found: {backup_dir}")
        return False
    
    success_count = 0
    
    # Apply Google LLM modifications
    if 'google_llm' in plugin_locations:
        try:
            google_backup = backup_dir / "google_llm_modified.py"
            if google_backup.exists():
                shutil.copy2(google_backup, plugin_locations['google_llm'])
                print(f"FRIDAY AI: Applied Google LLM modifications")
                success_count += 1
            else:
                print(f"FRIDAY AI: Error - Google LLM backup not found: {google_backup}")
        except Exception as e:
            print(f"FRIDAY AI: Error applying Google LLM modifications: {e}")
    
    # Apply Cartesia TTS modifications
    if 'cartesia_tts' in plugin_locations:
        try:
            cartesia_backup = backup_dir / "cartesia_tts_modified.py"
            if cartesia_backup.exists():
                shutil.copy2(cartesia_backup, plugin_locations['cartesia_tts'])
                print(f"FRIDAY AI: Applied Cartesia TTS modifications")
                success_count += 1
            else:
                print(f"FRIDAY AI: Error - Cartesia TTS backup not found: {cartesia_backup}")
        except Exception as e:
            print(f"FRIDAY AI: Error applying Cartesia TTS modifications: {e}")
    
    return success_count == len(plugin_locations)


def verify_modifications(plugin_locations):
    """Verify that modifications were applied correctly"""
    
    verification_passed = True
    
    for plugin_name, plugin_path in plugin_locations.items():
        try:
            with open(plugin_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Check for FRIDAY AI marker
            if "FRIDAY AI:" in content:
                print(f"FRIDAY AI: ✅ {plugin_name} modifications verified")
            else:
                print(f"FRIDAY AI: ❌ {plugin_name} modifications not found")
                verification_passed = False
                
            # Check for specific logging functions
            if plugin_name == 'google_llm' and "_log_user_message" in content:
                print(f"FRIDAY AI: ✅ Google LLM logging function found")
            elif plugin_name == 'cartesia_tts' and "_log_tts_message" in content:
                print(f"FRIDAY AI: ✅ Cartesia TTS logging function found")
            else:
                print(f"FRIDAY AI: ❌ {plugin_name} logging function not found")
                verification_passed = False
                
        except Exception as e:
            print(f"FRIDAY AI: Error verifying {plugin_name}: {e}")
            verification_passed = False
    
    return verification_passed


def main():
    """Main function to apply all modifications"""
    
    print("FRIDAY AI: Starting plugin modification process...")
    print("=" * 60)
    
    # Find plugin files
    plugin_locations = find_plugin_files()
    
    if not plugin_locations:
        print("FRIDAY AI: No plugin files found. This might be normal if plugins aren't installed yet.")
        return True
    
    print(f"FRIDAY AI: Found {len(plugin_locations)} plugin files to modify")
    
    # Backup original files
    print("\nFRIDAY AI: Creating backups of original files...")
    backup_original_files(plugin_locations)
    
    # Apply modifications
    print("\nFRIDAY AI: Applying conversation logging modifications...")
    if apply_modifications(plugin_locations):
        print("FRIDAY AI: ✅ All modifications applied successfully")
    else:
        print("FRIDAY AI: ❌ Some modifications failed")
        return False
    
    # Verify modifications
    print("\nFRIDAY AI: Verifying modifications...")
    if verify_modifications(plugin_locations):
        print("FRIDAY AI: ✅ All modifications verified successfully")
        print("FRIDAY AI: Plugin modification process completed!")
        return True
    else:
        print("FRIDAY AI: ❌ Verification failed")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)


----- File: C:\Users\int10281\Desktop\Github\Friday - Copy\docker_scripts\verify_modifications.py -----
#!/usr/bin/env python3
"""
FRIDAY AI: Verify Plugin Modifications Script for Docker Deployment
This script verifies that the conversation logging modifications were applied correctly.
"""

import os
import sys
import subprocess
from pathlib import Path


def verify_plugin_modifications():
    """Verify that plugin modifications are working correctly"""
    
    verification_results = {
        'google_llm': False,
        'cartesia_tts': False,
        'config_import': False,
        'logging_functions': False
    }
    
    try:
        # Find site-packages
        result = subprocess.run([
            sys.executable, "-c", 
            "import site; print(site.getsitepackages()[0])"
        ], capture_output=True, text=True, check=True)
        
        site_packages = Path(result.stdout.strip())
        print(f"FRIDAY AI: Checking plugins in: {site_packages}")
        
        # Check Google LLM
        google_llm_path = site_packages / "livekit" / "plugins" / "google" / "llm.py"
        if google_llm_path.exists():
            with open(google_llm_path, 'r', encoding='utf-8') as f:
                google_content = f.read()
            
            if "FRIDAY AI:" in google_content and "_log_user_message" in google_content:
                verification_results['google_llm'] = True
                print("FRIDAY AI: ✅ Google LLM modifications verified")
            else:
                print("FRIDAY AI: ❌ Google LLM modifications not found")
        else:
            print(f"FRIDAY AI: ❌ Google LLM file not found: {google_llm_path}")
        
        # Check Cartesia TTS
        cartesia_tts_path = site_packages / "livekit" / "plugins" / "cartesia" / "tts.py"
        if cartesia_tts_path.exists():
            with open(cartesia_tts_path, 'r', encoding='utf-8') as f:
                cartesia_content = f.read()
            
            if "FRIDAY AI:" in cartesia_content and "_log_tts_message" in cartesia_content:
                verification_results['cartesia_tts'] = True
                print("FRIDAY AI: ✅ Cartesia TTS modifications verified")
            else:
                print("FRIDAY AI: ❌ Cartesia TTS modifications not found")
        else:
            print(f"FRIDAY AI: ❌ Cartesia TTS file not found: {cartesia_tts_path}")
    
    except Exception as e:
        print(f"FRIDAY AI: Error during verification: {e}")
        return False
    
    # Check if config.py is accessible
    try:
        sys.path.insert(0, '/app')
        import config
        config.setup_conversation_log()
        verification_results['config_import'] = True
        print("FRIDAY AI: ✅ Config module import and setup verified")
    except Exception as e:
        print(f"FRIDAY AI: ❌ Config module verification failed: {e}")
    
    # Test logging functions
    try:
        if verification_results['google_llm'] and verification_results['cartesia_tts']:
            # Try to import and test the modified plugins
            from livekit.plugins.google.llm import LLM as GoogleLLM
            from livekit.plugins.cartesia.tts import TTS as CartesiaTTS
            
            # Check if logging methods exist
            llm = GoogleLLM()
            tts = CartesiaTTS()
            
            if hasattr(llm, '_log_user_message') and hasattr(tts, '_log_tts_message'):
                verification_results['logging_functions'] = True
                print("FRIDAY AI: ✅ Logging functions accessible")
            else:
                print("FRIDAY AI: ❌ Logging functions not accessible")
    except Exception as e:
        print(f"FRIDAY AI: ❌ Logging function test failed: {e}")
    
    return verification_results


def test_conversation_logging():
    """Test the complete conversation logging workflow"""
    
    try:
        print("\nFRIDAY AI: Testing conversation logging workflow...")
        
        # Import modified plugins
        from livekit.plugins.google.llm import LLM as GoogleLLM
        from livekit.plugins.cartesia.tts import TTS as CartesiaTTS
        
        # Test user message logging
        llm = GoogleLLM()
        test_user_input = "Test user message"
        llm._log_user_message(test_user_input, "voice")
        print("FRIDAY AI: ✅ User message logging test passed")
        
        # Test agent response logging  
        tts = CartesiaTTS()
        test_agent_response = "Test agent response"
        tts._log_tts_message(test_agent_response)
        print("FRIDAY AI: ✅ Agent response logging test passed")
        
        # Check if conversation file was created
        import config
        log_path = config.get_conversation_log_path()
        if os.path.exists(log_path):
            print(f"FRIDAY AI: ✅ Conversation log created at: {log_path}")
            return True
        else:
            print(f"FRIDAY AI: ❌ Conversation log not found at: {log_path}")
            return False
            
    except Exception as e:
        print(f"FRIDAY AI: ❌ Conversation logging test failed: {e}")
        return False


def main():
    """Main verification function"""
    
    print("FRIDAY AI: Starting plugin modification verification...")
    print("=" * 60)
    
    # Verify modifications
    verification_results = verify_plugin_modifications()
    
    # Count successful verifications
    success_count = sum(verification_results.values())
    total_checks = len(verification_results)
    
    print(f"\nFRIDAY AI: Verification Results: {success_count}/{total_checks} checks passed")
    
    # Detailed results
    for check, passed in verification_results.items():
        status = "✅" if passed else "❌"
        print(f"  {status} {check}")
    
    # Test conversation logging if basic verifications passed
    if verification_results['google_llm'] and verification_results['cartesia_tts']:
        logging_test_passed = test_conversation_logging()
    else:
        logging_test_passed = False
        print("\nFRIDAY AI: Skipping logging test due to failed basic verifications")
    
    # Overall result
    overall_success = success_count == total_checks and logging_test_passed
    
    if overall_success:
        print("\nFRIDAY AI: ✅ All verifications passed! Plugin modifications are working correctly.")
    else:
        print("\nFRIDAY AI: ❌ Some verifications failed. Please check the plugin modifications.")
    
    return overall_success


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)


----- File: C:\Users\int10281\Desktop\Github\Friday - Copy\testing_plugins\__init__.py -----
# FRIDAY AI: Dummy LiveKit plugins package init
"""
Testing plugins package for FRIDAY AI conversation logging system
"""

__version__ = "0.1.0-testing"


----- File: C:\Users\int10281\Desktop\Github\Friday - Copy\testing_plugins\livekit\__init__.py -----
# FRIDAY AI: Dummy LiveKit package init
"""
Testing LiveKit package for FRIDAY AI conversation logging system
"""


----- File: C:\Users\int10281\Desktop\Github\Friday - Copy\testing_plugins\livekit\plugins\__init__.py -----
# FRIDAY AI: Dummy plugins package init
"""
Testing plugins package for FRIDAY AI conversation logging system
"""


----- File: C:\Users\int10281\Desktop\Github\Friday - Copy\testing_plugins\livekit\plugins\cartesia\tts.py -----
# FRIDAY AI: Dummy Cartesia TTS Plugin for Testing
# This is a minimal implementation for testing the conversation logging system

import json
import os
from datetime import datetime
import asyncio
import config  # Import our config module


class TTS:
    """Dummy Cartesia TTS class for testing conversation logging"""
    
    def __init__(self, voice_id="hindi-voice", language="hi"):
        self.voice_id = voice_id
        self.language = language
        print(f"FRIDAY AI: Dummy Cartesia TTS initialized - Voice: {voice_id}, Language: {language}")
    
    def _log_tts_message(self, agent_response):
        """
        FRIDAY AI: Log agent response to conversation JSON
        
        Args:
            agent_response (str): The complete agent response
        """
        try:
            # Get the conversation log file path from config
            log_file_path = config.get_conversation_log_path()
            
            # Create log entry
            log_entry = {
                "timestamp": datetime.now().isoformat(),
                "role": "agent",
                "content": agent_response,
                "output_type": "voice"
            }
            
            # Read existing log or create new
            if os.path.exists(log_file_path):
                with open(log_file_path, 'r', encoding='utf-8') as f:
                    conversation_log = json.load(f)
            else:
                conversation_log = {"conversation": []}
            
            # Append new entry
            conversation_log["conversation"].append(log_entry)
            
            # Write back to file
            with open(log_file_path, 'w', encoding='utf-8') as f:
                json.dump(conversation_log, f, ensure_ascii=False, indent=2)
                
            print(f"FRIDAY AI: Agent response logged - Content: {agent_response[:50]}...")
            
        except Exception as e:
            print(f"FRIDAY AI: Error logging agent response: {e}")
    
    async def asynthesize(self, text, **kwargs):
        """Dummy async synthesize function that logs agent response"""
        # FRIDAY AI: Log the complete agent response
        self._log_tts_message(text)
        
        # Simulate audio generation delay
        await asyncio.sleep(0.1)
        
        # Return dummy audio data
        dummy_audio = b"dummy_audio_data_for_testing"
        print(f"FRIDAY AI: Dummy TTS synthesized audio for: {text[:50]}...")
        return dummy_audio
    
    def synthesize(self, text, **kwargs):
        """Dummy sync synthesize function"""
        # FRIDAY AI: Log the complete agent response
        self._log_tts_message(text)
        
        # Return dummy audio data
        dummy_audio = b"dummy_audio_data_for_testing"
        print(f"FRIDAY AI: Dummy TTS synthesized audio for: {text[:50]}...")
        return dummy_audio
    
    async def synthesize_streaming(self, text_stream, **kwargs):
        """Dummy streaming synthesis"""
        collected_text = ""
        
        # Collect all text chunks
        async for chunk in text_stream:
            collected_text += chunk
            yield b"dummy_audio_chunk"
        
        # FRIDAY AI: Log the complete agent response after collecting all chunks
        if collected_text.strip():
            self._log_tts_message(collected_text)


class TTSVoice:
    """Dummy TTS Voice class"""
    
    def __init__(self, voice_id="hindi-voice", name="Hindi Voice"):
        self.voice_id = voice_id
        self.name = name


# Mock functions for compatibility
def create_cartesia_tts(**kwargs):
    """Create a dummy Cartesia TTS instance"""
    return TTS(**kwargs)


def get_voices():
    """Return dummy voice list"""
    return [
        TTSVoice("hindi-voice", "Hindi Voice"),
        TTSVoice("english-voice", "English Voice")
    ]


if __name__ == "__main__":
    # Test the dummy TTS
    async def test_tts():
        tts = TTS()
        test_text = "नमस्ते, यह एक परीक्षण संदेश है।"
        audio = await tts.asynthesize(test_text)
        print(f"Test complete - Input: {test_text}, Audio length: {len(audio)} bytes")
    
    # Run the test
    asyncio.run(test_tts())


----- File: C:\Users\int10281\Desktop\Github\Friday - Copy\testing_plugins\livekit\plugins\cartesia\__init__.py -----
# FRIDAY AI: Dummy Cartesia plugin package init
"""
Testing Cartesia plugin package for FRIDAY AI conversation logging system
"""

from .tts import TTS, TTSVoice, create_cartesia_tts, get_voices

__all__ = ["TTS", "TTSVoice", "create_cartesia_tts", "get_voices"]


----- File: C:\Users\int10281\Desktop\Github\Friday - Copy\testing_plugins\livekit\plugins\google\llm.py -----
# FRIDAY AI: Dummy Google LLM Plugin for Testing
# This is a minimal implementation for testing the conversation logging system

import json
import os
from datetime import datetime
import config  # Import our config module


class LLM:
    """Dummy Google LLM class for testing conversation logging"""
    
    def __init__(self, model="gemini-1.5-flash"):
        self.model = model
        print(f"FRIDAY AI: Dummy Google LLM initialized with model: {model}")
    
    def _log_user_message(self, user_input, input_type="voice"):
        """
        FRIDAY AI: Log user message to conversation JSON
        
        Args:
            user_input (str): The user's input message
            input_type (str): Type of input - 'voice' or 'text'
        """
        try:
            # Get the conversation log file path from config
            log_file_path = config.get_conversation_log_path()
            
            # Create log entry
            log_entry = {
                "timestamp": datetime.now().isoformat(),
                "role": "user",
                "content": user_input,
                "input_type": input_type
            }
            
            # Read existing log or create new
            if os.path.exists(log_file_path):
                with open(log_file_path, 'r', encoding='utf-8') as f:
                    conversation_log = json.load(f)
            else:
                conversation_log = {"conversation": []}
            
            # Append new entry
            conversation_log["conversation"].append(log_entry)
            
            # Write back to file
            with open(log_file_path, 'w', encoding='utf-8') as f:
                json.dump(conversation_log, f, ensure_ascii=False, indent=2)
                
            print(f"FRIDAY AI: User message logged - Type: {input_type}, Content: {user_input[:50]}...")
            
        except Exception as e:
            print(f"FRIDAY AI: Error logging user message: {e}")
    
    async def agenerate(self, prompt, **kwargs):
        """Dummy generate function that logs user input"""
        # FRIDAY AI: Log the user input (prompt)
        self._log_user_message(prompt, "voice")  # Assuming voice input for testing
        
        # Return a dummy response
        dummy_response = "यह एक परीक्षण उत्तर है। (This is a test response.)"
        print(f"FRIDAY AI: Dummy LLM generated response: {dummy_response}")
        return dummy_response
    
    def generate(self, prompt, **kwargs):
        """Dummy generate function (sync version)"""
        # FRIDAY AI: Log the user input (prompt)
        self._log_user_message(prompt, "text")  # Assuming text input for sync
        
        # Return a dummy response
        dummy_response = "यह एक परीक्षण उत्तर है। (This is a test response.)"
        print(f"FRIDAY AI: Dummy LLM generated response: {dummy_response}")
        return dummy_response


# Mock functions for compatibility
def create_google_llm(**kwargs):
    """Create a dummy Google LLM instance"""
    return LLM(**kwargs)


if __name__ == "__main__":
    # Test the dummy LLM
    llm = LLM()
    test_prompt = "नमस्ते, आप कैसे हैं?"
    response = llm.generate(test_prompt)
    print(f"Test complete - Input: {test_prompt}, Output: {response}")


----- File: C:\Users\int10281\Desktop\Github\Friday - Copy\testing_plugins\livekit\plugins\google\__init__.py -----
# FRIDAY AI: Dummy Google plugin package init
"""
Testing Google plugin package for FRIDAY AI conversation logging system
"""

from .llm import LLM, create_google_llm

__all__ = ["LLM", "create_google_llm"]
